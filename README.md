# WhisperX Web Service para RTX 5080

Servicio web de transcripciÃ³n de audio usando WhisperX optimizado para NVIDIA RTX 5080 (16GB VRAM) con soporte para API REST y interfaz web.

## ğŸš€ CaracterÃ­sticas

- âœ… **Optimizado para RTX 5080** con CUDA 12.8
- âœ… **WhisperX**: Motor de transcripciÃ³n de alta precisiÃ³n
- âœ… **DiarizaciÃ³n**: IdentificaciÃ³n de mÃºltiples hablantes
- âœ… **MÃºltiples modelos**: Desde tiny hasta large-v3
- âœ… **API REST**: IntegraciÃ³n fÃ¡cil con tus aplicaciones
- âœ… **Interfaz Web**: Prueba y usa directamente desde el navegador
- âœ… **Docker**: Despliegue simplificado con contenedores

## ğŸ“‹ Requisitos Previos

- **Docker** y **Docker Compose** instalados
- **NVIDIA Docker Runtime** (nvidia-docker2)
- **GPU**: RTX 5080 o similar con 16GB VRAM
- **Drivers NVIDIA**: VersiÃ³n compatible con CUDA 12.8+
- **Token de Hugging Face**: Para usar diarizaciÃ³n con WhisperX

### Verificar instalaciÃ³n de Docker y NVIDIA

```bash
# Verificar Docker
docker --version
docker compose version

# Verificar soporte GPU
docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu22.04 nvidia-smi
```

## ğŸ”§ ConfiguraciÃ³n RÃ¡pida

### Paso 1: Configurar Variables de Entorno

1. **ObtÃ©n tu token de Hugging Face**:
   - Ve a https://huggingface.co/settings/tokens
   - Crea un token de lectura (Read)
   - Copia el token

2. **Crea el archivo `.env`**:

```bash
# Copiar el archivo de ejemplo
cp .env.example .env

# Editar y agregar tu token
nano .env  # o usa tu editor favorito
```

3. **Configura tu token en `.env`**:

```bash
HF_TOKEN=tu_token_de_huggingface_aqui
```

### Paso 2: Construir la Imagen Docker

```bash
docker build -t whisperx-rtx5080:latest -f Dockerfile.gpu .
```

Este proceso puede tardar 10-15 minutos la primera vez.

### Paso 3: Iniciar el Servicio

```bash
docker compose -f docker-compose.gpu.yml up -d
```

### Paso 4: Verificar que Funciona

```bash
# Ver logs
docker compose -f docker-compose.gpu.yml logs -f

# Verificar contenedor activo
docker ps
```

DeberÃ­as ver el contenedor `whisperx5080-whisper-asr-webservice-gpu-1` corriendo.

### Paso 5: Acceder al Servicio

- **Interfaz Web**: http://localhost:8000
- **DocumentaciÃ³n API**: http://localhost:8000/docs
- **API Alternativa**: http://localhost:8000/redoc

## ğŸ“Š ConfiguraciÃ³n de Modelos

### Modelos Disponibles y Uso de VRAM

| Modelo | VRAM (float16) | Velocidad | Calidad | Recomendado para |
|--------|----------------|-----------|---------|------------------|
| tiny | ~1GB | Muy rÃ¡pida | BÃ¡sica | Pruebas rÃ¡pidas |
| base | ~1.5GB | RÃ¡pida | Buena | Transcripciones simples |
| small | ~2.5GB | Media | Buena | Uso general |
| medium | ~5GB | Media-Lenta | Muy buena | **Balance Ã³ptimo** |
| large-v2 | ~8GB | Lenta | Excelente | Alta precisiÃ³n |
| large-v3 | ~10GB | Lenta | Excelente | **MÃ¡xima calidad** |

### Cambiar el Modelo

Edita `docker-compose.gpu.yml` y cambia:

```yaml
environment:
  - ASR_MODEL=medium  # Cambia aquÃ­: tiny, base, small, medium, large-v3
```

Luego reinicia:

```bash
docker compose -f docker-compose.gpu.yml restart
```

## ğŸ¯ Uso de la API

### Transcribir un Audio

```bash
curl -X POST "http://localhost:8000/asr" \
  -H "accept: text/plain" \
  -H "Content-Type: multipart/form-data" \
  -F "audio_file=@tu_audio.mp3" \
  -F "task=transcribe" \
  -F "language=es" \
  -F "output=json"
```

### Detectar Idioma

```bash
curl -X POST "http://localhost:8000/detect-language" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "audio_file=@tu_audio.mp3"
```

### Transcribir con DiarizaciÃ³n (MÃºltiples Hablantes)

```bash
curl -X POST "http://localhost:8000/asr" \
  -H "accept: text/plain" \
  -H "Content-Type: multipart/form-data" \
  -F "audio_file=@tu_audio.mp3" \
  -F "task=transcribe" \
  -F "language=es" \
  -F "diarize=true" \
  -F "min_speakers=2" \
  -F "max_speakers=5" \
  -F "output=json"
```

### Formatos de Salida Soportados

- `txt`: Texto plano
- `json`: JSON con timestamps
- `srt`: SubtÃ­tulos SubRip
- `vtt`: WebVTT subtÃ­tulos
- `tsv`: Valores separados por tabulaciÃ³n

## ğŸ” SoluciÃ³n de Problemas

### Error: "No se puede conectar al daemon de Docker"

```bash
sudo systemctl start docker
sudo usermod -aG docker $USER
# Cerrar sesiÃ³n y volver a entrar
```

### Error: "could not select device driver"

```bash
# Instalar NVIDIA Container Toolkit
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Error: "You must set the HF_TOKEN environment variable"

AsegÃºrate de tener el token en tu archivo `.env`:

```bash
HF_TOKEN=hf_tu_token_aqui
```

### El contenedor se detiene inmediatamente

```bash
# Ver logs para identificar el error
docker compose -f docker-compose.gpu.yml logs

# Errores comunes:
# - Token HF invÃ¡lido o faltante
# - GPU no detectada
# - Falta de memoria VRAM
```

### Problemas de memoria VRAM

Si te quedas sin memoria, prueba:

1. **Usar modelo mÃ¡s pequeÃ±o**: Cambiar a `medium` o `small`
2. **Habilitar cuantizaciÃ³n int8**:
   ```yaml
   environment:
     - ASR_QUANTIZATION=int8
   ```

## ğŸ› ï¸ Comandos Ãštiles

```bash
# Detener el servicio
docker compose -f docker-compose.gpu.yml down

# Ver logs en tiempo real
docker compose -f docker-compose.gpu.yml logs -f

# Reconstruir despuÃ©s de cambios
docker compose -f docker-compose.gpu.yml up -d --build

# Limpiar cachÃ© de modelos
docker volume rm whisperx5080_cache-whisper

# Entrar al contenedor
docker exec -it whisperx5080-whisper-asr-webservice-gpu-1 bash

# Ver uso de GPU
nvidia-smi

# Monitorear GPU en tiempo real
watch -n 1 nvidia-smi
```

## ğŸ“ˆ Optimizaciones de Rendimiento

### Para RTX 5080 (16GB)

La configuraciÃ³n actual ya estÃ¡ optimizada con:

- âœ… `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512`
- âœ… `CUDA_MODULE_LOADING=LAZY`
- âœ… CuantizaciÃ³n `float16` por defecto
- âœ… LÃ­mite de memoria del contenedor a 14GB

### Si necesitas mÃ¡s velocidad:

```yaml
environment:
  - ASR_MODEL=small  # Modelo mÃ¡s rÃ¡pido
  - ASR_QUANTIZATION=int8  # CuantizaciÃ³n mÃ¡s agresiva
```

### Si necesitas mÃ¡s calidad:

```yaml
environment:
  - ASR_MODEL=large-v3  # Mejor modelo
  - ASR_QUANTIZATION=float16  # Mayor precisiÃ³n
```

## ğŸŒ Idiomas Soportados

WhisperX soporta 99+ idiomas, incluyendo:

- ğŸ‡ªğŸ‡¸ EspaÃ±ol (es)
- ğŸ‡ºğŸ‡¸ InglÃ©s (en)
- ğŸ‡«ğŸ‡· FrancÃ©s (fr)
- ğŸ‡©ğŸ‡ª AlemÃ¡n (de)
- ğŸ‡®ğŸ‡¹ Italiano (it)
- ğŸ‡µğŸ‡¹ PortuguÃ©s (pt)
- ğŸ‡¨ğŸ‡³ Chino (zh)
- ğŸ‡¯ğŸ‡µ JaponÃ©s (ja)
- ğŸ‡°ğŸ‡· Coreano (ko)
- Y muchos mÃ¡s...

Ver lista completa en: http://localhost:8000/docs

## ğŸ“ Variables de Entorno Disponibles

| Variable | Valores | Por Defecto | DescripciÃ³n |
|----------|---------|-------------|-------------|
| `HF_TOKEN` | string | - | Token de Hugging Face (obligatorio) |
| `ASR_ENGINE` | whisperx, faster_whisper, openai_whisper | whisperx | Motor de transcripciÃ³n |
| `ASR_MODEL` | tiny, base, small, medium, large-v3 | large-v3 | Modelo a usar |
| `ASR_DEVICE` | cuda, cpu | cuda | Dispositivo de cÃ³mputo |
| `ASR_QUANTIZATION` | float32, float16, int8 | float16 | PrecisiÃ³n del modelo |
| `MODEL_IDLE_TIMEOUT` | nÃºmero | 0 | Tiempo antes de descargar modelo (segundos) |

Ver `.env.example` para configuraciÃ³n completa.

## ğŸ“š DocumentaciÃ³n Adicional

- [DocumentaciÃ³n de WhisperX](https://github.com/m-bain/whisperX)
- [Modelos de Whisper](https://github.com/openai/whisper#available-models-and-languages)
- [NVIDIA Docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)

## ğŸ“„ Licencia

MIT License - Ver archivo [LICENCE](LICENCE)

## ğŸ¤ Contribuciones

Basado en [whisper-asr-webservice](https://github.com/ahmetoner/whisper-asr-webservice) por Ahmet Ã–ner.

Optimizado para RTX 5080 por la comunidad.

## ğŸ’¡ Soporte

Si encuentras problemas:

1. Revisa la secciÃ³n **SoluciÃ³n de Problemas**
2. Verifica los logs: `docker compose -f docker-compose.gpu.yml logs`
3. Verifica que tu GPU sea detectada: `nvidia-smi`
4. AsegÃºrate de tener el token HF configurado

---

**Â¡Disfruta transcribiendo con WhisperX en tu RTX 5080! ğŸ‰**
